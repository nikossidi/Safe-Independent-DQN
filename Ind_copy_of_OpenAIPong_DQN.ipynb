{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikossidi/Safe-Independent-DQN/blob/main/Ind_copy_of_OpenAIPong_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4AS7njD7iL6"
      },
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip install \"gym[atari, accept-rom-license]\"\n",
        "!nvidia-smi\n",
        "# the skeleton of the below code is obtained from the project: https://github.com/bhctsntrk/OpenAIPong-DQN/blob/master/README.md #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D98CddQuwKG"
      },
      "source": [
        "import gym\n",
        "import cv2\n",
        "\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8rGMlN2uzgk"
      },
      "source": [
        "ENVIRONMENT = \"PongDeterministic-v4\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SAVE_MODELS = True  # Save models to file so you can test later\n",
        "MODEL_PATH = \"./pong-cnn-\"  # Models path for saving or loading\n",
        "SAVE_MODEL_INTERVAL = 10  # Save models at every X epoch\n",
        "TRAIN_MODEL = True  # Train model while playing (Make it False when testing a model)\n",
        "\n",
        "LOAD_MODEL_FROM_FILE = False  # Load model from file\n",
        "LOAD_FILE_EPISODE = 0  # Load Xth episode from file\n",
        "\n",
        "BATCH_SIZE = 32  # Minibatch size that select randomly from mem for train nets\n",
        "MAX_EPISODE = 501  # Max episode\n",
        "MAX_STEP = 100000  # Max step size for one episode\n",
        "\n",
        "MAX_MEMORY_LEN = 50000  # Max memory len\n",
        "MIN_MEMORY_LEN = 40000  # Min memory len before start train\n",
        "\n",
        "GAMMA = 0.99  # Discount rate\n",
        "ALPHA = 0.00025  # Learning rate\n",
        "EPSILON_DECAY = 0.995  # Epsilon decay rate by step\n",
        "\n",
        "RENDER_GAME_WINDOW = False  # Opens a new window to render the game (Won't work on colab default)\n",
        "\n",
        "# Plotting data\n",
        "episode_rewards = []\n",
        "episode_steps = []\n",
        "paddle_hits = []\n",
        "serving_times = []\n",
        "episode_durations = []\n",
        "mean_last_100_ep_reward = []\n",
        "mean_last_100_ep_steps = []\n",
        "mean_last_100_paddle_hits = []\n",
        "mean_last_100_ep_durations =[]\n",
        "mean_last_100_serving_times =[]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxF5-bzUu1q-"
      },
      "source": [
        "class DuelCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN with Duel Algo. https://arxiv.org/abs/1511.06581\n",
        "    \"\"\"\n",
        "    def __init__(self, h, w, output_size):\n",
        "        super(DuelCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        convw, convh = self.conv2d_size_calc(w, h, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=3, stride=1)\n",
        "\n",
        "        linear_input_size = convw * convh * 64  # Last conv layer's out sizes\n",
        "\n",
        "        # Action layer\n",
        "        self.Alinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.Alrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
        "        self.Alinear2 = nn.Linear(in_features=128, out_features=output_size)\n",
        "\n",
        "        # State Value layer\n",
        "        self.Vlinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.Vlrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
        "        self.Vlinear2 = nn.Linear(in_features=128, out_features=1)  # Only 1 node\n",
        "\n",
        "    def conv2d_size_calc(self, w, h, kernel_size=5, stride=2):\n",
        "        \"\"\"\n",
        "        Calcs conv layers output image sizes\n",
        "        \"\"\"\n",
        "        next_w = (w - (kernel_size - 1) - 1) // stride + 1\n",
        "        next_h = (h - (kernel_size - 1) - 1) // stride + 1\n",
        "        return next_w, next_h\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten every batch\n",
        "\n",
        "        Ax = self.Alrelu(self.Alinear1(x))\n",
        "        Ax = self.Alinear2(Ax)  # No activation on last layer\n",
        "\n",
        "        Vx = self.Vlrelu(self.Vlinear1(x))\n",
        "        Vx = self.Vlinear2(Vx)  # No activation on last layer\n",
        "\n",
        "        q = Vx + (Ax - Ax.mean())\n",
        "\n",
        "        return q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plT51MPbu5U5"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, environment):\n",
        "        \"\"\"\n",
        "        Hyperparameters definition for Agent\n",
        "        \"\"\"\n",
        "        # State size for breakout env. SS images (210, 160, 3). Used as input size in network\n",
        "        self.state_size_h = environment.observation_space.shape[0]\n",
        "        self.state_size_w = environment.observation_space.shape[1]\n",
        "        self.state_size_c = environment.observation_space.shape[2]\n",
        "\n",
        "        # Activation size for breakout env. Used as output size in network\n",
        "        self.action_size = environment.action_space.n\n",
        "\n",
        "        # Image pre process params\n",
        "        self.target_h = 80  # Height after process\n",
        "        self.target_w = 64  # Widht after process\n",
        "\n",
        "        self.crop_dim = [20, self.state_size_h, 0, self.state_size_w]  # Cut 20 px from top to get rid of the score table\n",
        "\n",
        "        # Trust rate to our experiences\n",
        "        self.gamma = GAMMA  # Discount coef for future predictions\n",
        "        self.alpha = ALPHA  # Learning Rate\n",
        "\n",
        "        # After many experinces epsilon will be 0.05\n",
        "        # So we will do less Explore more Exploit\n",
        "        self.epsilon = 1  # Explore or Exploit\n",
        "        self.epsilon_decay = EPSILON_DECAY  # Adaptive Epsilon Decay Rate\n",
        "        self.epsilon_minimum = 0.05  # Minimum for Explore\n",
        "\n",
        "        # Deque holds replay mem.\n",
        "        self.memory = deque(maxlen=MAX_MEMORY_LEN)\n",
        "\n",
        "        # Create two model for DDQN algorithm\n",
        "        self.online_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
        "        self.target_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
        "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
        "        self.target_model.eval()\n",
        "\n",
        "        # Adam used as optimizer\n",
        "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=self.alpha)\n",
        "\n",
        "    def preProcess(self, image):\n",
        "        \"\"\"\n",
        "        Process image crop resize, grayscale and normalize the images\n",
        "        \"\"\"\n",
        "        frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # To grayscale\n",
        "        frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]  # Cut 20 px from top\n",
        "        frame = cv2.resize(frame, (self.target_w, self.target_h))  # Resize\n",
        "        frame = frame.reshape(self.target_w, self.target_h) / 255  # Normalize\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Get state and do action\n",
        "        Two option can be selectedd if explore select random action\n",
        "        if exploit ask nnet for action\n",
        "        \"\"\"\n",
        "\n",
        "        act_protocol = 'Explore' if random.uniform(0, 1) <= self.epsilon else 'Exploit'\n",
        "\n",
        "        if act_protocol == 'Explore':\n",
        "            action = random.randrange(self.action_size)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
        "                q_values = self.online_model.forward(state)  # (1, action_size)\n",
        "                action = torch.argmax(q_values).item()  # Returns the indices of the maximum value of all elements\n",
        "\n",
        "        return action\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Train neural nets with replay memory\n",
        "        returns loss and max_q val predicted from online_net\n",
        "        \"\"\"\n",
        "        if len(agent.memory) < MIN_MEMORY_LEN:\n",
        "            loss, max_q = [0, 0]\n",
        "            return loss, max_q\n",
        "        # We get out minibatch and turn it to numpy array\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.memory, BATCH_SIZE))\n",
        "\n",
        "        # Concat batches in one array\n",
        "        # (np.arr, np.arr) ==> np.BIGarr\n",
        "        state = np.concatenate(state)\n",
        "        next_state = np.concatenate(next_state)\n",
        "\n",
        "        # Convert them to tensors\n",
        "        state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
        "        action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
        "        reward = torch.tensor(reward, dtype=torch.float, device=DEVICE)\n",
        "        done = torch.tensor(done, dtype=torch.float, device=DEVICE)\n",
        "\n",
        "        # Make predictions\n",
        "        state_q_values = self.online_model(state)\n",
        "        next_states_q_values = self.online_model(next_state)\n",
        "        next_states_target_q_values = self.target_model(next_state)\n",
        "\n",
        "        # Find selected action's q_value\n",
        "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "        # Get indice of the max value of next_states_q_values\n",
        "        # Use that indice to get a q_value from next_states_target_q_values\n",
        "        # We use greedy for policy So it called off-policy\n",
        "        next_states_target_q_value = next_states_target_q_values.gather(1, next_states_q_values.max(1)[1].unsqueeze(1)).squeeze(1)\n",
        "        # Use Bellman function to find expected q value\n",
        "        expected_q_value = reward + self.gamma * next_states_target_q_value * (1 - done)\n",
        "\n",
        "        # Calc loss with expected_q_value and q_value\n",
        "        loss = (selected_q_value - expected_q_value.detach()).pow(2).mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, torch.max(state_q_values).item()\n",
        "\n",
        "    def storeResults(self, state, action, reward, nextState, done):\n",
        "        \"\"\"\n",
        "        Store every result to memory\n",
        "        \"\"\"\n",
        "        self.memory.append([state[None, :], action, reward, nextState[None, :], done])\n",
        "\n",
        "    def adaptiveEpsilon(self):\n",
        "        \"\"\"\n",
        "        Adaptive Epsilon means every step\n",
        "        we decrease the epsilon so we do less Explore\n",
        "        \"\"\"\n",
        "        if self.epsilon > self.epsilon_minimum:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve4vYDe3bozg"
      },
      "source": [
        "environment = gym.make(ENVIRONMENT)  # Get env\n",
        "agent = Agent(environment)  # Create Agent\n",
        "\n",
        "if LOAD_MODEL_FROM_FILE:\n",
        "    agent.online_model.load_state_dict(torch.load(MODEL_PATH+str(LOAD_FILE_EPISODE)+\".pkl\"))\n",
        "\n",
        "    with open(MODEL_PATH+str(LOAD_FILE_EPISODE)+'.json') as outfile:\n",
        "        param = json.load(outfile)\n",
        "        agent.epsilon = param.get('epsilon')\n",
        "\n",
        "    startEpisode = LOAD_FILE_EPISODE + 1\n",
        "\n",
        "else:\n",
        "    startEpisode = 1\n",
        "\n",
        "last_100_ep_reward = deque(maxlen=100)  # Last 100 episode rewards\n",
        "last_100_ep_steps = deque(maxlen=100)  # Last 100 episode steps\n",
        "last_100_paddle_hits = deque(maxlen=100)  # Last 100 paddle hits\n",
        "last_100_ep_durations = deque(maxlen=100)  # Last 100 episode durations\n",
        "last_100_serving_times = deque(maxlen=100)  # Last 100 serving times\n",
        "total_step = 1  # Cumulkative sum of all steps in episodes\n",
        "\n",
        "# Adjusted reward for each time step\n",
        "REWARD_STEP = 0.001\n",
        "REWARD_SCALE_FACTOR = 0.01 # Scale factor for logarithmic function\n",
        "for episode in range(startEpisode, MAX_EPISODE):\n",
        "\n",
        "    startTime = time.time()  # Keep time\n",
        "    serving_start_time = None  # Initialize serving start time\n",
        "    paddle_hit_count = 0  # Initialize paddle hit count\n",
        "    state = environment.reset()  # Reset env\n",
        "\n",
        "    state = agent.preProcess(state)  # Process image\n",
        "\n",
        "    # Stack state . Every state contains 4 time contionusly frames\n",
        "    # We stack frames like 4 channel image\n",
        "    state = np.stack((state, state, state, state))\n",
        "\n",
        "    total_max_q_val = 0  # Total max q vals\n",
        "    total_reward = 0  # Total reward for each episode\n",
        "    total_loss = 0  # Total loss for each episode\n",
        "    episode_duration = 0  # Initialize episode duration\n",
        "    serving_start_time = None  # Initialize serving start time\n",
        "    for step in range(MAX_STEP):\n",
        "\n",
        "        if RENDER_GAME_WINDOW:\n",
        "            environment.render()  # Show state visually\n",
        "\n",
        "        # Select and perform an action\n",
        "        action = agent.act(state)  # Act\n",
        "        next_state, reward, done, info = environment.step(action)  # Observe\n",
        "        # Calculate reward using logarithmic function\n",
        "        reward = reward + (math.log(REWARD_STEP * step + 1) * REWARD_SCALE_FACTOR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Count paddle hits\n",
        "        if action in [1,3,5]:  # Actions correspond to move paddle up\n",
        "            paddle_hit_count += 1\n",
        "\n",
        "        # Check if the ball has been served and if the agent hits it for the first time\n",
        "        if serving_start_time is None and action in [1, 4, 5]:  # Actions correspond to serving the ball\n",
        "            serving_start_time = time.time()\n",
        "\n",
        "\n",
        "        # Track serving time\n",
        "        if serving_start_time is None:\n",
        "            serving_start_time = time.time()\n",
        "\n",
        "        next_state = agent.preProcess(next_state)  # Process image\n",
        "\n",
        "        # Stack state . Every state contains 4 time contionusly frames\n",
        "        # We stack frames like 4 channel image\n",
        "        next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
        "\n",
        "        # Store the transition in memory\n",
        "        agent.storeResults(state, action, reward, next_state, done)  # Store to mem\n",
        "\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state  # Update state\n",
        "\n",
        "        if TRAIN_MODEL:\n",
        "            # Perform one step of the optimization (on the target network)\n",
        "            loss, max_q_val = agent.train()  # Train with random BATCH_SIZE state taken from mem\n",
        "        else:\n",
        "            loss, max_q_val = [0, 0]\n",
        "\n",
        "        total_loss += loss\n",
        "        total_max_q_val += max_q_val\n",
        "        total_reward += reward\n",
        "        total_step += 1\n",
        "        if total_step % 1000 == 0:\n",
        "            agent.adaptiveEpsilon()  # Decrase epsilon\n",
        "\n",
        "        if done:  # Episode completed\n",
        "            currentTime = time.time()  # Keep current time\n",
        "            time_passed = currentTime - startTime  # Find episode duration\n",
        "            current_time_format = time.strftime(\"%H:%M:%S\", time.gmtime())  # Get current dateTime as HH:MM:SS\n",
        "            epsilonDict = {'epsilon': agent.epsilon}  # Create epsilon dict to save model as file\n",
        "\n",
        "            if SAVE_MODELS and episode % SAVE_MODEL_INTERVAL == 0:  # Save model as file\n",
        "                weightsPath = MODEL_PATH + str(episode) + '.pkl'\n",
        "                epsilonPath = MODEL_PATH + str(episode) + '.json'\n",
        "\n",
        "                torch.save(agent.online_model.state_dict(), weightsPath)\n",
        "                with open(epsilonPath, 'w') as outfile:\n",
        "                    json.dump(epsilonDict, outfile)\n",
        "\n",
        "            if TRAIN_MODEL:\n",
        "                agent.target_model.load_state_dict(agent.online_model.state_dict())  # Update target model\n",
        "\n",
        "            last_100_ep_reward.append(total_reward)\n",
        "            last_100_ep_steps.append(episode_steps)\n",
        "            last_100_paddle_hits.append(paddle_hit_count)\n",
        "            last_100_ep_durations.append(time_passed)\n",
        "            last_100_serving_times.append(serving_start_time - startTime)  # Measure serving time from start to first action\n",
        "\n",
        "            avg_max_q_val = total_max_q_val / step\n",
        "\n",
        "            mean_reward = np.mean(last_100_ep_reward)\n",
        "            mean_steps = np.mean(last_100_ep_steps)\n",
        "            mean_paddle_hits = np.mean(last_100_paddle_hits)\n",
        "            mean_durations = np.mean(last_100_ep_durations)\n",
        "            mean_serving_times = np.mean(last_100_serving_times)\n",
        "\n",
        "            outStr = \"Episode:{} Time:{} Reward:{:.2f} Loss:{:.2f} Last_100_Avg_Rew:{:.3f} Avg_Max_Q:{:.3f} Epsilon:{:.2f} Duration:{:.2f} Last_100_Avg_Duration:{:.3f} Step:{} Last_100_Avg_Steps:{:.3f} CStep:{} PaddleHits:{} Last_100_Avg_Paddle_Hits:{:.3f} Serving time:{} Last_100_Avg_Serving_Times:{:.3f}\".format(\n",
        "                episode, current_time_format, total_reward, total_loss, np.mean(last_100_ep_reward), avg_max_q_val, agent.epsilon, time_passed, np.mean(last_100_ep_durations), step, np.mean(last_100_ep_steps), total_step, paddle_hit_count,np.mean(last_100_paddle_hits), serving_start_time, np.mean(last_100_serving_times)\n",
        "            )\n",
        "\n",
        "            print(outStr)\n",
        "\n",
        "            if SAVE_MODELS:\n",
        "                outputPath = MODEL_PATH + \"out\" + '.txt'  # Save outStr to file\n",
        "                with open(outputPath, 'a') as outfile:\n",
        "                    outfile.write(outStr+\"\\n\")\n",
        "            # Collect data for plotting\n",
        "            episode_rewards.append(total_reward)\n",
        "            episode_steps.append(step)\n",
        "            paddle_hits.append(paddle_hit_count)\n",
        "            serving_times.append(serving_start_time - startTime)\n",
        "            episode_durations.append(time_passed)\n",
        "            mean_last_100_ep_reward.append(mean_reward)\n",
        "            mean_last_100_ep_steps.append(mean_steps)\n",
        "            mean_last_100_paddle_hits.append(mean_paddle_hits)\n",
        "            mean_last_100_ep_durations.append(mean_durations)\n",
        "            mean_last_100_serving_times.append(mean_serving_times)\n",
        "\n",
        "            break\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(episode_rewards, label = 'Episode rewards', c='b')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Reward per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Reward per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_ep_reward, label = 'Mean of Last 100 Episode Rewards', c='b')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode Rewards')\n",
        "plt.title('Mean of Last 100 Episode Rewards per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Mean of Last 100 Episode Rewards per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(episode_steps, label = 'Episode steps', c='b')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Step')\n",
        "plt.title('Step per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Step per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_ep_steps, label = 'Mean of Last 100 Episode steps', c='b')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode steps')\n",
        "plt.title('Mean of Last 100 Episode steps per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Mean of Last 100 Episode steps per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(paddle_hits, label ='Paddle hits', c='b')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Number of Paddle Hits')\n",
        "plt.title('Paddle Hits per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Paddle Hits per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_paddle_hits, label ='Mean of Last 100 Episode Paddle hits', c='b')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode Paddle hits')\n",
        "plt.title('Mean of Last 100 Episode Paddle hits per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Mean of Last 100 Episode Paddle hits per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(episode_durations, label = 'Episode durations', c='b')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Duration (seconds)')\n",
        "plt.title('Duration per Episode (seconds)')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Duration per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_ep_durations, label = 'Mean of Last 100 Episode durations', c='b')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode durations (seconds)')\n",
        "plt.title('Mean of Last 100 Episode durations per Episode (seconds)')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Mean of Last 100 Episode durations.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(serving_times, label ='serving_times', c='b')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Serving Time (seconds)')\n",
        "plt.title('Serving Time per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Serving Time per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_serving_times, label ='Mean of Last 100 Episode serving_times', c='b')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode serving_times')\n",
        "plt.title('Mean of Last 100 Episode serving_times')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Mean of Last 100 Episode serving_times.png')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VxycOAwE8JGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Export data to Pandas Dataframe\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(list(zip(episode_rewards,mean_last_100_ep_reward, episode_steps, mean_last_100_ep_steps, paddle_hits,mean_last_100_paddle_hits, episode_durations, mean_last_100_ep_durations, serving_times, mean_last_100_serving_times)),\n",
        "               columns =['episode_rewards', 'mean_last_100_ep_reward', 'episode_steps', 'mean_last_100_ep_steps', 'paddle_hits','mean_last_100_paddle_hits','episode_durations','mean_last_100_ep_durations', 'serving_times', 'mean_last_100_serving_times'])\n",
        "df.to_csv('/content/gdrive/MyDrive/Second_Method.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "nBf59yxcH8Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import data from first method\n",
        "plot_data=pd.read_csv('/content/gdrive/MyDrive/First_Method.csv')\n",
        "First_Method_episode_rewards=plot_data['episode_rewards'].to_list()\n",
        "First_Method_mean_last_100_ep_reward=plot_data['mean_last_100_ep_reward'].to_list()\n",
        "First_Method_episode_steps=plot_data['episode_steps'].to_list()\n",
        "First_Method_mean_last_100_ep_steps=plot_data['mean_last_100_ep_steps'].to_list()\n",
        "First_Method_paddle_hits=plot_data['paddle_hits'].to_list()\n",
        "First_Method_mean_last_100_paddle_hits=plot_data['mean_last_100_paddle_hits'].to_list()\n",
        "First_Method_episode_durations=plot_data['episode_durations'].to_list()\n",
        "First_Method_mean_last_100_ep_durations=plot_data['mean_last_100_ep_durations'].to_list()\n",
        "First_Method_serving_times=plot_data['serving_times'].to_list()\n",
        "First_Method_mean_last_100_serving_times=plot_data['mean_last_100_serving_times'].to_list()"
      ],
      "metadata": {
        "id": "u6vSZIVOQ43v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting comparison\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(episode_rewards, label = \"Safe Independent DQN\", c='b')\n",
        "plt.plot(First_Method_episode_rewards, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Reward per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Reward per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_ep_reward, label = 'Safe Independent DQN', c='b')\n",
        "plt.plot(First_Method_mean_last_100_ep_reward, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode Rewards')\n",
        "plt.title('Mean of Last 100 Episode Rewards per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Episode Rewards per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(episode_steps, label = \"Safe Independent DQN\", c='b')\n",
        "plt.plot(First_Method_episode_steps, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Step')\n",
        "plt.title('Step per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison steps per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_ep_steps, label = 'Safe Independent DQN', c='b')\n",
        "plt.plot(First_Method_mean_last_100_ep_steps, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode steps')\n",
        "plt.title('Mean of Last 100 Episode steps per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Episode steps per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(paddle_hits, label = \"Safe Independent DQN\", c='b')\n",
        "plt.plot(First_Method_paddle_hits, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Number of Paddle Hits')\n",
        "plt.title('Paddle Hits per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Paddle Hits per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_paddle_hits, label = 'Safe Independent DQN', c='b')\n",
        "plt.plot(First_Method_mean_last_100_paddle_hits, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode Paddle Hits')\n",
        "plt.title('Mean of Last 100 Episode Paddle Hits per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Episode Paddle Hits per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(episode_durations, label = \"Safe Independent DQN\", c='b')\n",
        "plt.plot(First_Method_episode_durations, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Duration (seconds)')\n",
        "plt.title('Duration per Episode (seconds)')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Duration per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_ep_durations, label = 'Safe Independent DQN', c='b')\n",
        "plt.plot(First_Method_mean_last_100_ep_durations, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Duration (seconds)')\n",
        "plt.title('Mean of Last 100 Duration (seconds) per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Duration (seconds) per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(serving_times, label = \"Safe Independent DQN\", c='b')\n",
        "plt.plot(First_Method_serving_times, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Serving Time (seconds)')\n",
        "plt.title('Serving Time per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Serving time per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_serving_times, label = \"Safe Independent DQN\", c='b')\n",
        "plt.plot(First_Method_mean_last_100_serving_times, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Serving Time (seconds)')\n",
        "plt.title('Mean of Last 100 Serving Time (seconds) per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Serving Time (seconds) per Episode.png')"
      ],
      "metadata": {
        "id": "wryGa-FdSUq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Statistics\n",
        "\n",
        "# Calculate mean and median and st dev\n",
        "mean_reward = np.mean(episode_rewards)\n",
        "median_reward = np.median(episode_rewards)\n",
        "std_dev_reward = np.std(episode_rewards)\n",
        "\n",
        "mean_duration = np.mean(episode_durations)\n",
        "median_duration = np.median(episode_durations)\n",
        "std_dev_duration = np.std(episode_durations)\n",
        "\n",
        "mean_paddle_hits = np.mean(paddle_hits)\n",
        "median_paddle_hits = np.median(paddle_hits)\n",
        "std_dev_paddle_hits = np.std(paddle_hits)\n",
        "\n",
        "mean_episode_steps = np.mean(episode_steps)\n",
        "median_episode_steps = np.median(episode_steps)\n",
        "std_dev_episode_steps = np.std(episode_steps)\n",
        "\n",
        "mean_serving_time = np.mean(serving_times)\n",
        "median_serving_time = np.median(serving_times)\n",
        "std_dev_serving_time = np.std(serving_times)\n",
        "\n",
        "# Print mean, median, and standard deviation values\n",
        "print(\"Mean Reward:\", mean_duration)\n",
        "print(\"Median Reward:\", median_duration)\n",
        "print(\"Standard Deviation Reward:\", std_dev_duration)\n",
        "\n",
        "print(\"Mean Duration:\", mean_duration)\n",
        "print(\"Median Duration:\", median_duration)\n",
        "print(\"Standard Deviation Duration:\", std_dev_duration)\n",
        "\n",
        "print(\"Mean Paddle Hits:\", mean_paddle_hits)\n",
        "print(\"Median Paddle Hits:\", median_paddle_hits)\n",
        "print(\"Standard Deviation Paddle Hits:\", std_dev_paddle_hits)\n",
        "\n",
        "print(\"Mean Episode Steps:\", mean_episode_steps)\n",
        "print(\"Median Episode Steps:\", median_episode_steps)\n",
        "print(\"Standard Deviation Episode Steps:\", std_dev_episode_steps)\n",
        "\n",
        "print(\"Mean Serving Time:\", mean_serving_time)\n",
        "print(\"Median Serving Time:\", median_serving_time)\n",
        "print(\"Standard Deviation Serving Time:\", std_dev_serving_time)"
      ],
      "metadata": {
        "id": "wMP2z1lCJENY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot only first 400 episodes\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_ep_reward[:400], label='Safe Independent DQN')\n",
        "plt.plot(First_Method_mean_last_100_ep_reward[:400], label=\"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode Rewards')\n",
        "plt.title('Mean of Last 100 Episode Rewards per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Episode Rewards per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_ep_steps[:400], label='Safe Independent DQN')\n",
        "plt.plot(First_Method_mean_last_100_ep_steps[:400], label=\"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode steps')\n",
        "plt.title('Mean of Last 100 Episode steps per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Episode steps per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_paddle_hits[:400], label='Safe Independent DQN')\n",
        "plt.plot(First_Method_mean_last_100_paddle_hits[:400], label=\"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode Paddle Hits')\n",
        "plt.title('Mean of Last 100 Episode Paddle Hits per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Episode Paddle Hits per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_ep_durations[:400], label='Safe Independent DQN')\n",
        "plt.plot(First_Method_mean_last_100_ep_durations[:400], label=\"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Duration (seconds)')\n",
        "plt.title('Mean of Last 100 Duration (seconds) per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Duration (seconds) per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_serving_times[:400], label=\"Safe Independent DQN\")\n",
        "plt.plot(First_Method_mean_last_100_serving_times[:400], label=\"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Serving Time (seconds)')\n",
        "plt.title('Mean of Last 100 Serving Time (seconds) per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Serving Time (seconds) per Episode.png')\n",
        "\n"
      ],
      "metadata": {
        "id": "UhHK5xYEfRYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do Not Use\n",
        "#Import data from first method trial 6\n",
        "plot_data=pd.read_csv('/content/gdrive/MyDrive/First_Method_trial6.csv')\n",
        "First_Method_episode_rewards_trial6=plot_data['episode_rewards'].to_list()\n",
        "First_Method_mean_last_100_ep_reward_trial6=plot_data['mean_last_100_ep_reward'].to_list()\n",
        "First_Method_episode_steps_trial6=plot_data['episode_steps'].to_list()\n",
        "First_Method_mean_last_100_ep_steps_trial6=plot_data['mean_last_100_ep_steps'].to_list()\n",
        "First_Method_paddle_hits_trial6=plot_data['paddle_hits'].to_list()\n",
        "First_Method_mean_last_100_paddle_hits_trial6=plot_data['mean_last_100_paddle_hits'].to_list()\n",
        "First_Method_episode_durations_trial6=plot_data['episode_durations'].to_list()\n",
        "First_Method_mean_last_100_ep_durations_trial6=plot_data['mean_last_100_ep_durations'].to_list()\n",
        "First_Method_serving_times_trial6=plot_data['serving_times'].to_list()\n",
        "First_Method_mean_last_100_serving_times_trial6=plot_data['mean_last_100_serving_times'].to_list()"
      ],
      "metadata": {
        "id": "2V6Z7QvIG0M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do Not Use\n",
        "# Plotting comparison with trial6\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(episode_rewards, label = \"Safe Independent DQN\", c='b')\n",
        "plt.plot(First_Method_episode_rewards_trial6, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Reward per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Reward per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_ep_reward, label = 'Safe Independent DQN', c='b')\n",
        "plt.plot(First_Method_mean_last_100_ep_reward_trial6, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode Rewards')\n",
        "plt.title('Mean of Last 100 Episode Rewards per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Episode Rewards per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(episode_steps, label = \"Safe Independent DQN\", c='b')\n",
        "plt.plot(First_Method_episode_steps_trial6, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Step')\n",
        "plt.title('Step per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison steps per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_ep_steps, label = 'Safe Independent DQN', c='b')\n",
        "plt.plot(First_Method_mean_last_100_ep_steps_trial6, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode steps')\n",
        "plt.title('Mean of Last 100 Episode steps per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Episode steps per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(paddle_hits, label = \"Safe Independent DQN\", c='b')\n",
        "plt.plot(First_Method_paddle_hits_trial6, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Number of Paddle Hits')\n",
        "plt.title('Paddle Hits per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Paddle Hits per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_paddle_hits, label = 'Safe Independent DQN', c='b')\n",
        "plt.plot(First_Method_mean_last_100_paddle_hits_trial6, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Episode Paddle Hits')\n",
        "plt.title('Mean of Last 100 Episode Paddle Hits per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Episode Paddle Hits per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(episode_durations, label = \"Safe Independent DQN\", c='b')\n",
        "plt.plot(First_Method_episode_durations_trial6, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Duration (seconds)')\n",
        "plt.title('Duration per Episode (seconds)')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Duration per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_ep_durations, label = 'Safe Independent DQN', c='b')\n",
        "plt.plot(First_Method_mean_last_100_ep_durations_trial6, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Duration (seconds)')\n",
        "plt.title('Mean of Last 100 Duration (seconds) per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Duration (seconds) per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(serving_times, label = \"Safe Independent DQN\", c='b')\n",
        "plt.plot(First_Method_serving_times_trial6, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Serving Time (seconds)')\n",
        "plt.title('Serving Time per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Serving time per Episode.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(mean_last_100_serving_times, label = \"Safe Independent DQN\", c='b')\n",
        "plt.plot(First_Method_mean_last_100_serving_times_trial6, label = \"Basic DQN\", c='r')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean of Last 100 Serving Time (seconds)')\n",
        "plt.title('Mean of Last 100 Serving Time (seconds) per Episode')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.savefig('Comparison Mean of Last 100 Serving Time (seconds) per Episode.png')"
      ],
      "metadata": {
        "id": "WDyd52cUHFR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}